#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - Enhanced Module Processor
Processador de m√≥dulos que trabalha com o JSON gigante
"""
import os
import logging
import json
import re
from datetime import datetime
from typing import Dict, List, Any, Optional
from collections import Counter
from services.ai_manager import ai_manager
from services.auto_save_manager import salvar_etapa

logger = logging.getLogger(__name__)

class EnhancedModuleProcessor:
    """Processador aprimorado que usa dados do JSON gigante"""
    def __init__(self):
        """Inicializa o processador de m√≥dulos aprimorado"""
        self.required_modules = [
            'avatars',
            'drivers_mentais',
            'anti_objecao',
            'provas_visuais',
            'pre_pitch',
            'predicoes_futuro',
            'posicionamento',
            'concorrencia',
            'palavras_chave',
            'funil_vendas',
            'insights',
            'plano_acao'
        ]
        self.processed_modules = {}
        # Stopwords b√°sicas em portugu√™s para an√°lise de texto
        self.stopwords = {
            "de", "a", "o", "que", "e", "do", "da", "em", "um", "para", "com", "n√£o", "uma", "os", "no", "se",
            "na", "por", "mais", "as", "dos", "como", "mas", "ao", "ele", "das", "seu", "sua", "ou", "quando",
            "muito", "nos", "j√°", "eu", "tamb√©m", "s√≥", "pelo", "pela", "at√©", "isso", "ela", "entre", "era",
            "depois", "sem", "mesmo", "aos", "seus", "quem", "nas", "me", "esse", "eles", "voc√™", "essa",
            "num", "nem", "suas", "meu", "√†s", "minha", "numa", "pelos", "elas", "qual", "n√≥s", "lhe",
            "deles", "essas", "esses", "pelas", "este", "dele", "tu", "te", "voc√™s", "vos", "lhes", "meus",
            "minhas", "teu", "tua", "teus", "tuas", "nosso", "nossa", "nossos", "nossas", "dela", "delas",
            "esta", "estes", "estas", "aquele", "aquela", "aqueles", "aquelas", "isto", "aquilo", "estou",
            "est√°", "estamos", "est√£o", "estive", "esteve", "estivemos", "estiveram", "estava", "est√°vamos",
            "estavam", "estivera", "estiv√©ramos", "esteja", "estejamos", "estejam", "estivesse", "estiv√©ssemos",
            "estivessem", "estiver", "estivermos", "estiverem", "hei", "h√°", "havemos", "h√£o", "houve",
            "houvemos", "houveram", "houvera", "houv√©ramos", "haja", "hajamos", "hajam", "houvesse",
            "houv√©ssemos", "houvessem", "houver", "houvermos", "houverem", "houverei", "houver√°", "houveremos",
            "houver√£o", "houveria", "houver√≠amos", "houveriam", "sou", "somos", "s√£o", "era", "√©ramos",
            "eram", "fui", "foi", "fomos", "foram", "fora", "f√¥ramos", "seja", "sejamos", "sejam", "fosse",
            "f√¥ssemos", "fossem", "for", "formos", "forem", "serei", "ser√°", "seremos", "ser√£o", "seria",
            "ser√≠amos", "seriam", "tenho", "tem", "temos", "t√©m", "tinha", "t√≠nhamos", "tinham", "tive",
            "teve", "tivemos", "tiveram", "tivera", "tiv√©ramos", "tenha", "tenhamos", "tenham", "tivesse",
            "tiv√©ssemos", "tivessem", "tiver", "tivermos", "tiverem", "terei", "ter√°", "teremos", "ter√£o",
            "teria", "ter√≠amos", "teriam"
        }
        logger.info("üîß Enhanced Module Processor inicializado")

    def process_all_modules_from_massive_data(
        self,
        massive_data: Dict[str, Any],
        context: Dict[str, Any],
        session_id: str
    ) -> Dict[str, Any]:
        """Processa todos os m√≥dulos usando dados do JSON gigante"""
        logger.info("üöÄ INICIANDO PROCESSAMENTO DE TODOS OS M√ìDULOS COM DADOS MASSIVOS")
        processing_results = {
            "session_id": session_id,
            "processing_started": datetime.now().isoformat(),
            "modules_data": {},
            "processing_summary": {},
            "massive_data_stats": massive_data.get("statistics", {})
        }
        # Processa cada m√≥dulo sequencialmente
        for module_name in self.required_modules:
            try:
                logger.info(f"üì¶ Processando m√≥dulo: {module_name}")
                module_result = self._process_single_module(
                    module_name, massive_data, context, session_id
                )
                if module_result:
                    processing_results["modules_data"][module_name] = module_result
                    self._save_module_json(module_name, module_result, session_id)
                    logger.info(f"‚úÖ M√≥dulo {module_name} processado com sucesso")
                else:
                    logger.error(f"‚ùå Falha ao processar m√≥dulo {module_name}")
            except Exception as e:
                logger.error(f"‚ùå Erro no m√≥dulo {module_name}: {e}")
                processing_results["modules_data"][module_name] = {"error": str(e)}

        # Gera sum√°rio do processamento
        processing_results["processing_summary"] = self._generate_processing_summary(processing_results)
        processing_results["processing_completed"] = datetime.now().isoformat()
        logger.info(f"‚úÖ PROCESSAMENTO COMPLETO: {len(processing_results['modules_data'])} m√≥dulos")
        return processing_results

    def _process_single_module(
        self,
        module_name: str,
        massive_data: Dict[str, Any],
        context: Dict[str, Any],
        session_id: str
    ) -> Optional[Dict[str, Any]]:
        """Processa um √∫nico m√≥dulo usando dados massivos"""
        # Mapeia m√©todo de processamento para cada m√≥dulo
        processing_methods = {
            'avatars': self._process_avatars_module,
            'drivers_mentais': self._process_drivers_mentais_module,
            'anti_objecao': self._process_anti_objecao_module,
            'provas_visuais': self._process_provas_visuais_module,
            'pre_pitch': self._process_pre_pitch_module,
            'predicoes_futuro': self._process_predicoes_futuro_module,
            'posicionamento': self._process_posicionamento_module,
            'concorrencia': self._process_concorrencia_module,
            'palavras_chave': self._process_palavras_chave_module,
            'funil_vendas': self._process_funil_vendas_module,
            'insights': self._process_insights_module,
            'plano_acao': self._process_plano_acao_module
        }
        if module_name in processing_methods:
            return processing_methods[module_name](massive_data, context, session_id)
        else:
            logger.error(f"‚ùå M√©todo de processamento n√£o encontrado para {module_name}")
            return None

    def _process_avatars_module(self, massive_data: Dict[str, Any], context: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Processa m√≥dulo de avatars usando dados massivos"""
        # Extrai dados relevantes do JSON gigante
        social_insights = self._extract_social_insights(massive_data)
        web_insights = self._extract_web_insights(massive_data)
        content_analysis = self._extract_content_analysis(massive_data)
        avatar_prompt = f"""
        Baseado nos dados massivos coletados, crie um avatar ultra-detalhado para o segmento "{context.get('segmento', '')}" e produto "{context.get('produto', '')}".
        DADOS COLETADOS:
        REDES SOCIAIS:
        - Total de posts analisados: {social_insights.get('total_posts', 0)}
        - Plataformas ativas: {social_insights.get('platforms_active', 0)}
        - Sentimento geral: {social_insights.get('sentiment', 'neutro')}
        - T√≥picos trending: {social_insights.get('trending_topics', [])}
        - Coment√°rios mais frequentes: {social_insights.get('frequent_comments', [])}
        PESQUISA WEB:
        - Total de fontes web: {web_insights.get('total_sources', 0)}
        - Qualidade m√©dia das fontes: {web_insights.get('avg_quality', 0)}
        - Principais dom√≠nios: {web_insights.get('top_domains', [])}
        AN√ÅLISE DE CONTE√öDO:
        - Total de conte√∫do extra√≠do: {content_analysis.get('total_content_length', 0)} caracteres
        - N√∫mero de documentos: {content_analysis.get('documents_count', 0)}
        - Temas principais identificados: {content_analysis.get('main_themes', [])}
        - Palavras-chave mais relevantes: {content_analysis.get('key_terms', [])}
        Crie um avatar que inclua:
        1. DEMOGRAFIA ULTRA-DETALHADA
           - Idade espec√≠fica e faixa
           - Localiza√ß√£o geogr√°fica
           - Renda e classe social
           - Educa√ß√£o e profiss√£o
           - Estado civil e fam√≠lia
        2. PSICOGRAFIA PROFUNDA
           - Valores e cren√ßas centrais
           - Medos e inseguran√ßas espec√≠ficos
           - Aspira√ß√µes e sonhos
           - Personalidade e comportamento
           - Estilo de vida e hobbies
        3. COMPORTAMENTO DIGITAL
           - Plataformas mais utilizadas
           - Hor√°rios de maior atividade
           - Tipo de conte√∫do consumido
           - Influenciadores seguidos
           - Padr√µes de compra online
        4. DORES E NECESSIDADES VISCERAIS
           - Principais problemas enfrentados
           - Consequ√™ncias dos problemas
           - Tentativas frustradas de solu√ß√£o
           - Gatilhos emocionais
           - Urg√™ncia das necessidades
        5. JORNADA DE COMPRA DETALHADA
           - Processo de tomada de decis√£o
           - Fontes de informa√ß√£o utilizadas
           - Obje√ß√µes mais comuns
           - Crit√©rios de escolha
           - Momentos de maior receptividade
        Responda em formato JSON estruturado.
        """
        try:
            avatar_result = ai_manager.generate_content(avatar_prompt, max_tokens=4000)
            # Tentar parsear o resultado como JSON
            try:
                parsed_result = json.loads(avatar_result)
            except json.JSONDecodeError:
                logger.error(f"‚ùå Avatar result is not valid JSON: {avatar_result[:200]}...")
                parsed_result = {"raw_output": avatar_result}

            return {
                "module": "avatars",
                "avatar_ultra_detalhado": parsed_result,
                "data_sources_used": {
                    "social_posts": social_insights.get('total_posts', 0),
                    "web_sources": web_insights.get('total_sources', 0),
                    "content_documents": content_analysis.get('documents_count', 0)
                },
                "insights_foundation": {
                    "social_insights": social_insights,
                    "web_insights": web_insights,
                    "content_analysis": content_analysis
                },
                "generated_at": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento do avatar: {e}")
            return {"error": str(e), "module": "avatars"}

    def _process_drivers_mentais_module(self, massive_data: Dict[str, Any], context: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Processa m√≥dulo de drivers mentais usando dados massivos"""
        content_insights = self._extract_detailed_content_insights(massive_data)
        psychological_patterns = self._extract_psychological_patterns(massive_data)
        drivers_prompt = f"""
        Baseado na an√°lise massiva de dados coletados, crie 19 drivers mentais personalizados para o segmento "{context.get('segmento', '')}" e produto "{context.get('produto', '')}".
        INSIGHTS DE CONTE√öDO MASSIVO:
        - Palavras-chave mais frequentes: {content_insights.get('top_keywords', [])}
        - Temas emocionais identificados: {content_insights.get('emotional_themes', [])}
        - Padr√µes de linguagem: {content_insights.get('language_patterns', [])}
        PADR√ïES PSICOL√ìGICOS IDENTIFICADOS:
        - Medos mais comuns: {psychological_patterns.get('common_fears', [])}
        - Desejos predominantes: {psychological_patterns.get('dominant_desires', [])}
        - Gatilhos de urg√™ncia: {psychological_patterns.get('urgency_triggers', [])}
        - Motivadores de a√ß√£o: {psychological_patterns.get('action_motivators', [])}
        Para cada um dos 19 drivers mentais, forne√ßa:
        1. NOME do driver
        2. GATILHO CENTRAL espec√≠fico
        3. DEFINI√á√ÉO VISCERAL que conecta emocionalmente
        4. APLICA√á√ÉO PR√ÅTICA no contexto do produto/segmento
        5. FRASES DE ANCORAGEM (3 frases prontas para usar)
        6. CONTEXTOS DE USO espec√≠ficos
        7. INTENSIDADE EMOCIONAL (escala 1-10)
        8. COMPATIBILIDADE com outros drivers
        Baseie os drivers nos insights reais extra√≠dos dos dados massivos coletados.
        Responda em formato JSON estruturado.
        """
        try:
            drivers_result = ai_manager.generate_content(drivers_prompt, max_tokens=5000)
            # Tentar parsear o resultado como JSON
            try:
                parsed_result = json.loads(drivers_result)
            except json.JSONDecodeError:
                logger.error(f"‚ùå Drivers result is not valid JSON: {drivers_result[:200]}...")
                parsed_result = {"raw_output": drivers_result}
            return {
                "module": "drivers_mentais",
                "drivers_mentais_arsenal": parsed_result,
                "data_foundation": {
                    "content_insights": content_insights,
                    "psychological_patterns": psychological_patterns,
                    "total_sources_analyzed": massive_data.get("statistics", {}).get("total_sources", 0)
                },
                "customization_level": "ULTRA_PERSONALIZADO",
                "generated_at": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento dos drivers: {e}")
            return {"error": str(e), "module": "drivers_mentais"}

    def _process_anti_objecao_module(self, massive_data: Dict[str, Any], context: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Processa m√≥dulo anti-obje√ß√£o usando dados massivos"""
        objections_patterns = self._extract_objection_patterns(massive_data)
        competitor_analysis = self._extract_competitor_insights(massive_data)
        anti_objection_prompt = f"""
        Baseado na an√°lise massiva de dados, crie um sistema anti-obje√ß√£o completo para "{context.get('segmento', '')}" vendendo "{context.get('produto', '')}".
        PADR√ïES DE OBJE√á√ïES IDENTIFICADOS:
        - Obje√ß√µes mais frequentes encontradas: {objections_patterns.get('common_objections', [])}
        - Preocupa√ß√µes do p√∫blico-alvo: {objections_patterns.get('target_concerns', [])}
        - Pontos de resist√™ncia: {objections_patterns.get('resistance_points', [])}
        AN√ÅLISE COMPETITIVA:
        - Como concorrentes lidam com obje√ß√µes: {competitor_analysis.get('competitor_approaches', [])}
        - Lacunas identificadas: {competitor_analysis.get('market_gaps', [])}
        - Oportunidades de diferencia√ß√£o: {competitor_analysis.get('differentiation_opportunities', [])}
        Crie um sistema que inclua:
        1. MAPEAMENTO COMPLETO DAS OBJE√á√ïES
           - Top 15 obje√ß√µes mais prov√°veis
           - Categoriza√ß√£o por tipo e intensidade
           - Momento prov√°vel de surgimento
        2. ESTRAT√âGIAS DE NEUTRALIZA√á√ÉO
           - T√©cnica espec√≠fica para cada obje√ß√£o
           - Scripts de resposta testados
           - Reframes poderosos
        3. PREVEN√á√ÉO PROATIVA
           - Como evitar que obje√ß√µes surjam
           - Elementos de credibilidade necess√°rios
           - Provas sociais espec√≠ficas
        4. SCRIPTS PERSONALIZADOS
           - Linguagem adaptada ao p√∫blico
           - Varia√ß√µes para diferentes contextos
           - T√©cnicas de fechamento p√≥s-obje√ß√£o
        Responda em formato JSON estruturado.
        """
        try:
            anti_objection_result = ai_manager.generate_content(anti_objection_prompt, max_tokens=4500)
            # Tentar parsear o resultado como JSON
            try:
                parsed_result = json.loads(anti_objection_result)
            except json.JSONDecodeError:
                logger.error(f"‚ùå Anti-objection result is not valid JSON: {anti_objection_result[:200]}...")
                parsed_result = {"raw_output": anti_objection_result}
            return {
                "module": "anti_objecao",
                "sistema_anti_objecao": parsed_result,
                "analysis_foundation": {
                    "objections_patterns": objections_patterns,
                    "competitor_analysis": competitor_analysis,
                    "data_sources": massive_data.get("statistics", {}).get("sources_by_type", {})
                },
                "coverage_level": "COMPLETA",
                "generated_at": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento anti-obje√ß√£o: {e}")
            return {"error": str(e), "module": "anti_objecao"}

    def _process_provas_visuais_module(self, massive_data: Dict[str, Any], context: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Processa m√≥dulo de provas visuais usando dados massivos"""
        visual_patterns = self._extract_visual_patterns(massive_data)
        engagement_data = self._extract_engagement_insights(massive_data)
        provas_visuais_prompt = f"""
        Baseado nos dados massivos coletados, crie um arsenal de provas visuais para "{context.get('segmento', '')}" e produto "{context.get('produto', '')}".
        PADR√ïES VISUAIS IDENTIFICADOS:
        - Tipos de conte√∫do com maior engajamento: {visual_patterns.get('high_engagement_content', [])}
        - Formatos visuais mais eficazes: {visual_patterns.get('effective_formats', [])}
        - Elementos visuais recorrentes: {visual_patterns.get('recurring_elements', [])}
        DADOS DE ENGAJAMENTO:
        - M√©tricas de performance por tipo: {engagement_data.get('performance_by_type', {})}
        - Hor√°rios de maior engajamento: {engagement_data.get('peak_hours', [])}
        - Plataformas mais responsivas: {engagement_data.get('responsive_platforms', [])}
        Crie 8 tipos de provas visuais:
        1. PROVA DE RESULTADO (antes/depois)
        2. PROVA SOCIAL (depoimentos visuais)
        3. PROVA DE AUTORIDADE (credenciais)
        4. PROVA DE URG√äNCIA (escassez visual)
        5. PROVA DE VALOR (compara√ß√µes)
        6. PROVA DE PROCESSO (demonstra√ß√µes)
        7. PROVA DE CREDIBILIDADE (certifica√ß√µes)
        8. PROVA EMOCIONAL (stories visuais)
        Para cada prova visual, forne√ßa:
        - Objetivo psicol√≥gico espec√≠fico
        - Elementos visuais necess√°rios
        - Copy sugerida
        - Contextos de uso ideais
        - M√©tricas de success esperadas
        - Varia√ß√µes para diferentes plataformas
        Responda em formato JSON estruturado.
        """
        try:
            provas_visuais_result = ai_manager.generate_content(provas_visuais_prompt, max_tokens=4000)
            # Tentar parsear o resultado como JSON
            try:
                parsed_result = json.loads(provas_visuais_result)
            except json.JSONDecodeError:
                logger.error(f"‚ùå Provas visuais result is not valid JSON: {provas_visuais_result[:200]}...")
                parsed_result = {"raw_output": provas_visuais_result}
            return {
                "module": "provas_visuais",
                "arsenal_provas_visuais": parsed_result,
                "visual_foundation": {
                    "visual_patterns": visual_patterns,
                    "engagement_data": engagement_data,
                    "platforms_analyzed": list(massive_data.get("social_media_data", {}).get("all_platforms_data", {}).get("platforms", {}).keys())
                },
                "customization_level": "ULTRA_SEGMENTADA",
                "generated_at": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento das provas visuais: {e}")
            return {"error": str(e), "module": "provas_visuais"}

    def _process_predicoes_futuro_module(self, massive_data: Dict[str, Any], context: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Processa m√≥dulo de predi√ß√µes futuras usando dados massivos"""
        trends_analysis = self._extract_trends_analysis(massive_data)
        market_signals = self._extract_market_signals(massive_data)
        predicoes_prompt = f"""
        Baseado na an√°lise massiva de dados coletados, crie predi√ß√µes futuras detalhadas para o mercado de "{context.get('segmento', '')}" e produto "{context.get('produto', '')}".
        AN√ÅLISE DE TEND√äNCIAS:
        - Tend√™ncias emergentes identificadas: {trends_analysis.get('emerging_trends', [])}
        - Padr√µes de crescimento: {trends_analysis.get('growth_patterns', [])}
        - Mudan√ßas comportamentais: {trends_analysis.get('behavioral_changes', [])}
        SINAIS DE MERCADO:
        - Indicadores de demanda: {market_signals.get('demand_indicators', [])}
        - Movimenta√ß√µes competitivas: {market_signals.get('competitive_movements', [])}
        - Fatores disruptivos: {market_signals.get('disruptive_factors', [])}
        Crie predi√ß√µes para:
        1. PR√ìXIMOS 6 MESES
           - Oportunidades imediatas
           - Amea√ßas a considerar
           - Movimentos estrat√©gicos recomendados
        2. PR√ìXIMO ANO
           - Mudan√ßas estruturais esperadas
           - Novos players no mercado
           - Evolu√ß√£o das necessidades do cliente
        3. PR√ìXIMOS 2-3 ANOS
           - Transforma√ß√µes do setor
           - Tecnologias disruptivas
           - Novos modelos de neg√≥cio
        4. CEN√ÅRIOS POSS√çVEIS
           - Melhor cen√°rio (otimista)
           - Cen√°rio mais prov√°vel (realista)
           - Pior cen√°rio (pessimista)
        Para cada predi√ß√£o, inclua:
        - Probabilidade de ocorr√™ncia (%)
        - Impacto no neg√≥cio (escala 1-10)
        - Sinais de confirma√ß√£o a observar
        - A√ß√µes preparat√≥rias recomendadas
        Responda em formato JSON estruturado.
        """
        try:
            predicoes_result = ai_manager.generate_content(predicoes_prompt, max_tokens=4500)
            # Tentar parsear o resultado como JSON
            try:
                parsed_result = json.loads(predicoes_result)
            except json.JSONDecodeError:
                logger.error(f"‚ùå Predi√ß√µes result is not valid JSON: {predicoes_result[:200]}...")
                parsed_result = {"raw_output": predicoes_result}
            return {
                "module": "predicoes_futuro",
                "predicoes_detalhadas": parsed_result,
                "analysis_foundation": {
                    "trends_analysis": trends_analysis,
                    "market_signals": market_signals,
                    "data_timespan": "Dados coletados em tempo real",
                    "confidence_level": "ALTO"
                },
                "prediction_horizon": "6_meses_a_3_anos",
                "generated_at": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento das predi√ß√µes: {e}")
            return {"error": str(e), "module": "predicoes_futuro"}

    # M√©todos auxiliares para extrair insights dos dados massivos
    def _extract_social_insights(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai insights das redes sociais"""
        social_data = massive_data.get("social_media_data", {})
        insights = {
            "total_posts": 0,
            "platforms_active": 0,
            "sentiment": "neutro",
            "trending_topics": [],
            "frequent_comments": []
        }
        # Dados das plataformas
        platforms_data = social_data.get("all_platforms_data", {}).get("platforms", {})
        insights["platforms_active"] = len([p for p in platforms_data.values() if p.get("results")])
        
        all_comments = []
        for platform_data in platforms_data.values():
            results = platform_data.get("results", [])
            insights["total_posts"] += len(results)
            for post in results:
                # Extrai coment√°rios se existirem
                comments = post.get("comments", [])
                all_comments.extend([c.get("text", "") for c in comments if isinstance(c, dict)])
        
        # An√°lise de sentimento
        sentiment_data = social_data.get("sentiment_analysis", {})
        if sentiment_data.get("overall_sentiment"):
            insights["sentiment"] = sentiment_data["overall_sentiment"]
        
        # Trending topics - usando palavras-chave de trending topics
        trending_data = social_data.get("trending_topics", {})
        keywords_freq = trending_data.get("keywords_frequency", {})
        insights["trending_topics"] = list(keywords_freq.keys())[:5]
        
        # Coment√°rios frequentes
        if all_comments:
            # Limpa e conta coment√°rios
            cleaned_comments = [re.sub(r'[^\w\s]', '', c.lower()) for c in all_comments if len(c.split()) > 2]
            comment_counter = Counter(cleaned_comments)
            insights["frequent_comments"] = [comment for comment, count in comment_counter.most_common(5)]

        return insights

    def _extract_web_insights(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai insights da busca web"""
        web_data = massive_data.get("web_search_data", {})
        insights = {
            "total_sources": 0,
            "avg_quality": 0,
            "top_domains": []
        }
        # Conta fontes
        enhanced_results = web_data.get("enhanced_search_results", {})
        exa_results = enhanced_results.get("exa_results", [])
        google_results = enhanced_results.get("google_results", [])
        insights["total_sources"] += len(exa_results)
        insights["total_sources"] += len(google_results)
        
        # Extrai dom√≠nios principais
        domains = []
        for result in exa_results:
            if result.get("url"):
                domain = result["url"].split("/")[2] if "/" in result["url"] else ""
                if domain:
                    domains.append(domain)
        insights["top_domains"] = list(set(domains))[:5]
        
        # Qualidade m√©dia (placeholder)
        insights["avg_quality"] = 7.5 # Valor estimado
        return insights

    def _extract_content_analysis(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai an√°lise do conte√∫do extra√≠do"""
        extracted_content = massive_data.get("extracted_content", [])
        analysis = {
            "documents_count": len(extracted_content),
            "total_content_length": massive_data.get("statistics", {}).get("total_content_length", 0),
            "main_themes": [],
            "key_terms": []
        }
        # An√°lise b√°sica de temas
        if extracted_content:
            all_content = " ".join([item.get("content", "")[:500] for item in extracted_content[:10]])
            words = re.findall(r'\b\w+\b', all_content.lower())
            # Filtra stopwords
            filtered_words = [word for word in words if len(word) > 4 and word not in self.stopwords]
            word_freq = Counter(filtered_words)
            sorted_words = word_freq.most_common(10)
            analysis["main_themes"] = [word for word, count in sorted_words]
            analysis["key_terms"] = [word for word, count in sorted_words[:5]]
        return analysis

    def _extract_detailed_content_insights(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai insights detalhados do conte√∫do"""
        extracted_content = massive_data.get("extracted_content", [])
        keywords = []
        emotional_themes = []
        language_patterns = []
        
        if extracted_content:
            combined_text = " ".join([item.get("content", "")[:1000] for item in extracted_content[:20]])
            words = re.findall(r'\b\w+\b', combined_text.lower())
            
            # Filtra stopwords e palavras curtas
            filtered_words = [word for word in words if len(word) > 3 and word not in self.stopwords]
            word_freq = Counter(filtered_words)
            top_keywords = [word for word, count in word_freq.most_common(15)]
            keywords.extend(top_keywords)
            
            # Detecta padr√µes de linguagem comuns
            common_patterns = ["como", "voc√™", "pode", "imagine", "descubra", "garanta", "agora", "hoje"]
            language_patterns = [p for p in common_patterns if p in combined_text.lower()]
            
            # Detecta temas emocionais comuns
            emotional_indicators = {
                "urg√™ncia": ["urgente", "agora", "hoje", "limitado", "√∫ltima chance"],
                "confian√ßa": ["confie", "garantia", "prova", "certeza"],
                "sucesso": ["sucesso", "transforma√ß√£o", "resultados", "atingir"],
                "transforma√ß√£o": ["transforma√ß√£o", "mudan√ßa", "evolu√ß√£o", "crescimento"]
            }
            for theme, indicators in emotional_indicators.items():
                if any(indicator in combined_text.lower() for indicator in indicators):
                    emotional_themes.append(theme)

        return {
            "top_keywords": list(set(keywords)),
            "emotional_themes": list(set(emotional_themes)),
            "language_patterns": list(set(language_patterns))
        }

    def _extract_psychological_patterns(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai padr√µes psicol√≥gicos dos dados"""
        social_data = massive_data.get("social_media_data", {})
        extracted_content = massive_data.get("extracted_content", [])
        
        common_fears = []
        dominant_desires = []
        urgency_triggers = []
        action_motivators = []
        
        # Analisa coment√°rios de redes sociais para medos e desejos
        platforms_data = social_data.get("all_platforms_data", {}).get("platforms", {})
        all_comments = []
        for platform_data in platforms_data.values():
            results = platform_data.get("results", [])
            for post in results:
                comments = post.get("comments", [])
                all_comments.extend([c.get("text", "") for c in comments if isinstance(c, dict)])
        
        combined_text = " ".join(all_comments[:1000]) + " " + " ".join([item.get("content", "")[:500] for item in extracted_content[:10]])
        
        # Detecta padr√µes psicol√≥gicos
        fear_indicators = {
            "fracasso": ["fracasso", "falhar", "erro", "n√£o consigo"],
            "perda": ["perder", "falta de", "sem", "n√£o ter"],
            "rejei√ß√£o": ["rejeitado", "criticado", "n√£o aceito"],
            "inadequa√ß√£o": ["n√£o sou", "n√£o posso", "incapaz"]
        }
        for fear, indicators in fear_indicators.items():
            if any(indicator in combined_text.lower() for indicator in indicators):
                common_fears.append(fear)
        
        desire_indicators = {
            "sucesso": ["sucesso", "atingir", "realizar"],
            "reconhecimento": ["reconhecido", "elogiado", "destaque"],
            "seguran√ßa": ["seguro", "protegido", "tranquilidade"],
            "liberdade": ["liberdade", "autonomia", "tempo"]
        }
        for desire, indicators in desire_indicators.items():
            if any(indicator in combined_text.lower() for indicator in indicators):
                dominant_desires.append(desire)
        
        # Gatilhos de urg√™ncia
        urgency_words = ["prazo", "√∫ltima", "chance", "acabar", "esgotar", "terminar", "acabando"]
        if any(word in combined_text.lower() for word in urgency_words):
            urgency_triggers.append("prazo limitado")
        if "√∫ltima" in combined_text.lower() or "ultima" in combined_text.lower():
            urgency_triggers.append("√∫ltimas vagas")
        if "oportunidade" in combined_text.lower():
            urgency_triggers.append("oportunidade √∫nica")
            
        # Motivadores de a√ß√£o
        motivator_indicators = {
            "garantia": ["garantia", "devolu√ß√£o", "teste"],
            "prova social": ["depoimento", "recomenda", "usu√°rio", "cliente"],
            "autoridade": ["especialista", "profissional", "certificado"],
            "escassez": ["limitado", "√∫nico", "exclusivo"]
        }
        for motivator, indicators in motivator_indicators.items():
            if any(indicator in combined_text.lower() for indicator in indicators):
                action_motivators.append(motivator)

        return {
            "common_fears": list(set(common_fears)),
            "dominant_desires": list(set(dominant_desires)),
            "urgency_triggers": list(set(urgency_triggers)),
            "action_motivators": list(set(action_motivators))
        }

    def _extract_objection_patterns(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai padr√µes de obje√ß√µes dos dados"""
        social_data = massive_data.get("social_media_data", {})
        extracted_content = massive_data.get("extracted_content", [])
        
        common_objections = []
        target_concerns = []
        resistance_points = []
        
        # Analisa coment√°rios de redes sociais para obje√ß√µes
        platforms_data = social_data.get("all_platforms_data", {}).get("platforms", {})
        all_comments = []
        for platform_data in platforms_data.values():
            results = platform_data.get("results", [])
            for post in results:
                comments = post.get("comments", [])
                all_comments.extend([c.get("text", "") for c in comments if isinstance(c, dict)])
        
        combined_text = " ".join(all_comments[:1000]) + " " + " ".join([item.get("content", "")[:500] for item in extracted_content[:10]])
        
        # Detecta obje√ß√µes comuns
        objection_indicators = {
            "muito caro": ["caro", "muito dinheiro", "n√£o posso pagar", "custo alto"],
            "n√£o tenho tempo": ["sem tempo", "n√£o d√° tempo", "ocupado", "correria"],
            "preciso pensar": ["preciso pensar", "vou ver", "depois eu vejo", "ainda n√£o sei"],
            "n√£o confio": ["n√£o confio", "d√∫vida", "desconfio", "golpe"]
        }
        for objection, indicators in objection_indicators.items():
            if any(indicator in combined_text.lower() for indicator in indicators):
                common_objections.append(objection)
        
        # Detecta preocupa√ß√µes do p√∫blico
        concern_indicators = {
            "qualidade": ["qualidade", "funciona", "durabilidade"],
            "resultados": ["resultado", "efic√°cia", "funcionar"],
            "suporte": ["suporte", "atendimento", "ajuda"],
            "garantias": ["garantia", "devolu√ß√£o", "seguran√ßa"]
        }
        for concern, indicators in concern_indicators.items():
            if any(indicator in combined_text.lower() for indicator in indicators):
                target_concerns.append(concern)
                
        # Detecta pontos de resist√™ncia
        resistance_indicators = {
            "pre√ßo": ["pre√ßo", "custo", "valor", "caro"],
            "complexidade": ["complicado", "dif√≠cil", "complexo", "entender"],
            "tempo": ["tempo", "demora", "lento"],
            "credibilidade": ["confian√ßa", "reputa√ß√£o", "empresa"]
        }
        for resistance, indicators in resistance_indicators.items():
            if any(indicator in combined_text.lower() for indicator in indicators):
                resistance_points.append(resistance)

        return {
            "common_objections": list(set(common_objections)),
            "target_concerns": list(set(target_concerns)),
            "resistance_points": list(set(resistance_points))
        }

    def _extract_competitor_insights(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai insights sobre concorrentes"""
        # Implementa√ß√£o b√°sica - pode ser expandida
        return {
            "competitor_approaches": ["pre√ßo baixo", "qualidade premium", "suporte 24h"],
            "market_gaps": ["atendimento personalizado", "entrega r√°pida", "garantia estendida"],
            "differentiation_opportunities": ["inova√ß√£o", "experi√™ncia", "valor agregado"]
        }

    def _extract_visual_patterns(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai padr√µes visuais dos dados"""
        # Implementa√ß√£o b√°sica - pode ser expandida
        return {
            "high_engagement_content": ["v√≠deos", "carross√©is", "stories", "infogr√°ficos"],
            "effective_formats": ["quadrado", "vertical", "horizontal"],
            "recurring_elements": ["cores vibrantes", "texto grande", "call-to-action claro"]
        }

    def _extract_engagement_insights(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai insights de engajamento"""
        # Implementa√ß√£o b√°sica - pode ser expandida
        return {
            "performance_by_type": {"video": 85, "image": 70, "text": 45},
            "peak_hours": ["19:00-21:00", "12:00-14:00", "08:00-09:00"],
            "responsive_platforms": ["instagram", "linkedin", "youtube"]
        }

    def _extract_trends_analysis(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai an√°lise de tend√™ncias"""
        # Implementa√ß√£o b√°sica - pode ser expandida
        return {
            "emerging_trends": ["digitaliza√ß√£o", "sustentabilidade", "personaliza√ß√£o"],
            "growth_patterns": ["crescimento exponencial", "ado√ß√£o gradual", "curva S"],
            "behavioral_changes": ["consumo online", "busca por conveni√™ncia", "valor da experi√™ncia"]
        }

    def _extract_market_signals(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai sinais de mercado"""
        # Implementa√ß√£o b√°sica - pode ser expandida
        return {
            "demand_indicators": ["aumento de buscas", "novos entrantes", "investimentos"],
            "competitive_movements": ["lan√ßamentos", "aquisi√ß√µes", "parcerias"],
            "disruptive_factors": ["tecnologia", "regulamenta√ß√£o", "mudan√ßa cultural"]
        }

    def _extract_market_positioning_insights(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai insights de posicionamento de mercado"""
        return {
            "positioning_trends": ["personaliza√ß√£o", "sustentabilidade", "conveni√™ncia"],
            "market_gaps": ["atendimento humanizado", "pre√ßo acess√≠vel", "qualidade premium"],
            "differentiation_opportunities": ["inova√ß√£o tecnol√≥gica", "experi√™ncia √∫nica", "valores aut√™nticos"]
        }

    def _extract_competitive_landscape(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai panorama competitivo"""
        return {
            "competitor_positions": ["l√≠der de pre√ßo", "premium quality", "inova√ß√£o tecnol√≥gica"],
            "differentiation_points": ["atendimento", "qualidade", "pre√ßo", "conveni√™ncia"],
            "unoccupied_spaces": ["nicho premium acess√≠vel", "sustentabilidade real", "simplicidade extrema"]
        }

    def _extract_competitive_data(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai dados competitivos espec√≠ficos"""
        return {
            "competitors_identified": ["concorrente A", "concorrente B", "concorrente C"],
            "competitive_strategies": ["diferencia√ß√£o", "lideran√ßa de custo", "foco"],
            "strengths_weaknesses": ["forte em marketing", "fraco em atendimento", "inovador mas caro"]
        }

    def _extract_market_dynamics(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai din√¢micas de mercado"""
        return {
            "recent_movements": ["fus√µes", "novos produtos", "mudan√ßas de pre√ßo"],
            "competitive_trends": ["digitaliza√ß√£o", "sustentabilidade", "personaliza√ß√£o"],
            "open_opportunities": ["mercado inexplorado", "nicho emergente", "necessidade n√£o atendida"]
        }

    def _extract_keyword_analysis(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai an√°lise de palavras-chave dos dados massivos"""
        extracted_content = massive_data.get("extracted_content", [])
        # Analisa conte√∫do real para extrair palavras-chave
        all_text = " ".join([item.get("content", "")[:1000] for item in extracted_content[:20]])
        words = re.findall(r'\b\w+\b', all_text.lower())
        # Filtra palavras relevantes (> 3 caracteres, n√£o stopwords b√°sicas)
        word_freq = Counter([word for word in words if len(word) > 3 and word not in self.stopwords])
        sorted_words = word_freq.most_common(30)
        frequent_terms = [word for word, count in sorted_words]
        return {
            "frequent_terms": frequent_terms,
            "high_intent_keywords": [f"{term} comprar" for term in frequent_terms[:10]],
            "longtail_keywords": [f"como {term} melhor", f"{term} profissional", f"{term} online"]
        }

    def _extract_search_patterns(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai padr√µes de busca"""
        return {
            "search_intents": ["informacional", "navegacional", "transacional", "comercial"],
            "search_journey": ["descoberta", "pesquisa", "compara√ß√£o", "compra"],
            "content_gaps": ["tutoriais", "compara√ß√µes", "reviews", "guias"]
        }

    def _extract_customer_journey_insights(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai insights da jornada do cliente"""
        return {
            "main_touchpoints": ["redes sociais", "site", "email", "atendimento"],
            "decision_moments": ["primeira impress√£o", "compara√ß√£o", "prova social", "garantia"],
            "conversion_barriers": ["pre√ßo", "confian√ßa", "complexidade", "tempo"]
        }

    def _extract_conversion_patterns(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai padr√µes de convers√£o"""
        return {
            "conversion_triggers": ["urg√™ncia", "escassez", "prova social", "garantia"],
            "persuasive_elements": ["depoimentos", "n√∫meros", "autoridade", "reciprocidade"],
            "conversion_optimizations": ["simplifica√ß√£o", "redu√ß√£o de passos", "clareza", "confian√ßa"]
        }

    def _extract_unique_insights(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai insights √∫nicos dos dados"""
        return {
            "behavioral_patterns": ["prefer√™ncia por visual", "busca por simplicidade", "valoriza experi√™ncia"],
            "unexpected_correlations": ["qualidade √ó pre√ßo", "conveni√™ncia √ó lealdade", "atendimento √ó recompra"],
            "emerging_trends": ["sustentabilidade", "personaliza√ß√£o", "imediatismo"]
        }

    def _extract_market_opportunities(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai oportunidades de mercado"""
        return {
            "unexplored_niches": ["p√∫blico jovem premium", "terceira idade digital", "B2B simplificado"],
            "unmet_needs": ["atendimento 24h", "entrega imediata", "personaliza√ß√£o total"],
            "competitive_gaps": ["pre√ßo justo + qualidade", "tecnologia + humaniza√ß√£o", "simplicidade + poder"]
        }

    def _extract_implementation_insights(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai insights de implementa√ß√£o"""
        return {
            "implementation_strategies": ["faseada", "piloto", "big bang", "incremental"],
            "success_timelines": ["3 meses setup", "6 meses valida√ß√£o", "12 meses escala"],
            "success_factors": ["equipe dedicada", "or√ßamento adequado", "m√©tricas claras"]
        }

    def _extract_resource_requirements(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai requisitos de recursos"""
        return {
            "typical_resources": ["equipe 3-5 pessoas", "or√ßamento marketing", "ferramentas tecnol√≥gicas"],
            "investment_ranges": ["setup: R$ 10-50k", "marketing: R$ 5-20k/m√™s", "opera√ß√£o: R$ 3-15k/m√™s"],
            "team_requirements": ["gerente projeto", "especialista marketing", "analista dados"]
        }

    def _extract_engagement_patterns(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai padr√µes de engajamento"""
        return {
            "high_engagement_elements": ["v√≠deos", "stories", "interatividade", "humor"],
            "peak_attention_moments": ["primeiros 3 segundos", "meio da apresenta√ß√£o", "call to action"],
            "effective_formats": ["v√≠deo curto", "carrossel", "live", "stories"]
        }

    def _extract_attention_triggers(self, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai gatilhos de aten√ß√£o"""
        return {
            "powerful_hooks": ["pergunta provocativa", "estat√≠stica chocante", "hist√≥ria pessoal"],
            "surprise_elements": ["reviravoltas", "dados inesperados", "demonstra√ß√µes"],
            "storytelling_techniques": ["her√≥i jornada", "antes/depois", "problema/solu√ß√£o"]
        }

    def _process_posicionamento_module(self, massive_data: Dict[str, Any], context: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Processa m√≥dulo de posicionamento usando dados massivos"""
        market_insights = self._extract_market_positioning_insights(massive_data)
        competitive_landscape = self._extract_competitive_landscape(massive_data)
        posicionamento_prompt = f"""
        Baseado na an√°lise massiva de dados coletados, crie um posicionamento estrat√©gico completo para "{context.get('produto', '')}" no segmento "{context.get('segmento', '')}".
        INSIGHTS DE MERCADO:
        - Tend√™ncias de posicionamento identificadas: {market_insights.get('positioning_trends', [])}
        - Lacunas de mercado detectadas: {market_insights.get('market_gaps', [])}
        - Oportunidades de diferencia√ß√£o: {market_insights.get('differentiation_opportunities', [])}
        CEN√ÅRIO COMPETITIVO:
        - Posicionamentos concorrentes: {competitive_landscape.get('competitor_positions', [])}
        - Pontos de diferencia√ß√£o dispon√≠veis: {competitive_landscape.get('differentiation_points', [])}
        - Espa√ßos n√£o ocupados: {competitive_landscape.get('unoccupied_spaces', [])}
        Crie um posicionamento que inclua:
        1. PROPOSTA DE VALOR √öNICA
           - Statement principal em uma frase
           - Benef√≠cios funcionais espec√≠ficos
           - Benef√≠cios emocionais √∫nicos
           - Raz√£o de acreditar concreta
        2. DIFERENCIA√á√ÉO COMPETITIVA
           - 3 pilares de diferencia√ß√£o
           - Vantagens competitivas sustent√°veis
           - Barreiras para imita√ß√£o
           - Proof points espec√≠ficos
        3. TERRIT√ìRIO DE MARCA
           - Personalidade da marca
           - Tom de voz espec√≠fico
           - Valores centrais
           - Miss√£o e vis√£o
        4. ARQUITETURA DE MENSAGEM
           - Headline principal
           - Subheadlines de apoio
           - Argumentos de venda √∫nicos
           - Call-to-actions otimizados
        5. ESTRAT√âGIA DE COMUNICA√á√ÉO
           - Canais priorit√°rios
           - Mensagens por canal
           - Cronograma de implementa√ß√£o
           - M√©tricas de sucesso
        Responda em formato JSON estruturado.
        """
        try:
            posicionamento_result = ai_manager.generate_content(posicionamento_prompt, max_tokens=4000)
            # Tentar parsear o resultado como JSON
            try:
                parsed_result = json.loads(posicionamento_result)
            except json.JSONDecodeError:
                logger.error(f"‚ùå Posicionamento result is not valid JSON: {posicionamento_result[:200]}...")
                parsed_result = {"raw_output": posicionamento_result}
            return {
                "module": "posicionamento",
                "posicionamento_estrategico": parsed_result,
                "market_foundation": {
                    "market_insights": market_insights,
                    "competitive_landscape": competitive_landscape,
                    "data_sources": massive_data.get("statistics", {}).get("total_sources", 0)
                },
                "produto": context.get('produto', ''),
                "segmento": context.get('segmento', ''),
                "generated_at": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento do posicionamento: {e}")
            return {"error": str(e), "module": "posicionamento"}

    def _process_concorrencia_module(self, massive_data: Dict[str, Any], context: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Processa m√≥dulo de an√°lise competitiva usando dados massivos"""
        competitive_data = self._extract_competitive_data(massive_data)
        market_dynamics = self._extract_market_dynamics(massive_data)
        concorrencia_prompt = f"""
        Baseado na an√°lise massiva de dados coletados, crie uma an√°lise competitiva completa para "{context.get('produto', '')}" no segmento "{context.get('segmento', '')}".
        DADOS COMPETITIVOS COLETADOS:
        - Concorrentes identificados: {competitive_data.get('competitors_identified', [])}
        - Estrat√©gias competitivas observadas: {competitive_data.get('competitive_strategies', [])}
        - Pontos fortes e fracos mapeados: {competitive_data.get('strengths_weaknesses', [])}
        DIN√ÇMICAS DE MERCADO:
        - Movimenta√ß√µes recentes: {market_dynamics.get('recent_movements', [])}
        - Tend√™ncias competitivas: {market_dynamics.get('competitive_trends', [])}
        - Oportunidades abertas: {market_dynamics.get('open_opportunities', [])}
        Crie uma an√°lise que inclua:
        1. MAPEAMENTO COMPETITIVO COMPLETO
           - Top 5 concorrentes diretos identificados
           - Top 3 concorrentes indiretos
           - Novos entrantes potenciais
           - Substitutos relevantes
        2. AN√ÅLISE SWOT DETALHADA
           - For√ßas espec√≠ficas de cada concorrente
           - Fraquezas explor√°veis identificadas
           - Oportunidades de mercado abertas
           - Amea√ßas competitivas iminentes
        3. MATRIZ DE POSICIONAMENTO
           - Posicionamento de cada player
           - Gaps de posicionamento dispon√≠veis
           - Espa√ßos super competitivos a evitar
           - Nichos inexplorados
        4. ESTRAT√âGIAS COMPETITIVAS
           - Como cada concorrente compete
           - T√°ticas de diferencia√ß√£o observadas
           - Pontos de vulnerabilidade
           - Oportunidades de ataque
        5. RECOMENDA√á√ïES ESTRAT√âGICAS
           - Estrat√©gia competitiva recomendada
           - Movimentos t√°ticos sugeridos
           - Cronograma de implementa√ß√£o
           - M√©tricas de monitoramento
        Responda em formato JSON estruturado.
        """
        try:
            concorrencia_result = ai_manager.generate_content(concorrencia_prompt, max_tokens=4500)
            # Tentar parsear o resultado como JSON
            try:
                parsed_result = json.loads(concorrencia_result)
            except json.JSONDecodeError:
                logger.error(f"‚ùå Concorr√™ncia result is not valid JSON: {concorrencia_result[:200]}...")
                parsed_result = {"raw_output": concorrencia_result}
            return {
                "module": "concorrencia",
                "analise_competitiva_completa": parsed_result,
                "competitive_foundation": {
                    "competitive_data": competitive_data,
                    "market_dynamics": market_dynamics,
                    "analysis_depth": "ULTRA_DETALHADA"
                },
                "produto": context.get('produto', ''),
                "segmento": context.get('segmento', ''),
                "generated_at": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento da an√°lise competitiva: {e}")
            return {"error": str(e), "module": "concorrencia"}

    def _process_palavras_chave_module(self, massive_data: Dict[str, Any], context: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Processa m√≥dulo de palavras-chave usando dados massivos"""
        keyword_analysis = self._extract_keyword_analysis(massive_data)
        search_patterns = self._extract_search_patterns(massive_data)
        # Extrai dados espec√≠ficos baseado no tipo de m√≥dulo
        context_data = self._extract_context_for_module(module_name='palavras_chave', massive_data=massive_data)
        # Prompt personalizado baseado no m√≥dulo
        # Define termo principal baseado no contexto
        termo_principal = context_data.get('produto', context_data.get('segmento', 'termo principal'))
        prompt = f"""
        Voc√™ √© um especialista em SEO e palavras-chave estrat√©gicas.
        Baseado nos dados massivos coletados, extraia e organize as palavras-chave mais estrat√©gicas para:
        Segmento: {context_data.get('segmento', 'N/A')}
        Produto: {context_data.get('produto', 'N/A')}
        Dados analisados: {json.dumps(context_data, ensure_ascii=False)[:2000]}
        Organize em:
        1. Palavras-chave prim√°rias (alto volume, alta relev√¢ncia)
        2. Palavras-chave de cauda longa
        3. Palavras-chave de inten√ß√£o comercial
        4. Palavras-chave da concorr√™ncia
        5. Oportunidades de nicho
        Para cada palavra-chave, inclua:
        - Volume de busca estimado
        - Dificuldade de rankeamento
        - Inten√ß√£o do usu√°rio
        - Sugest√µes de uso
        Retorne em formato JSON estruturado com todas as categorias.
        Termo principal analisado: {termo_principal}
        """
        try:
            palavras_chave_result = ai_manager.generate_content(prompt, max_tokens=4000)
            # Tentar parsear o resultado como JSON
            try:
                parsed_result = json.loads(palavras_chave_result)
            except json.JSONDecodeError:
                logger.error(f"‚ùå Palavras-chave result is not valid JSON: {palavras_chave_result[:200]}...")
                parsed_result = {"raw_output": palavras_chave_result}
            return {
                "module": "palavras_chave",
                "estrategia_palavras_chave": parsed_result,
                "keyword_foundation": {
                    "keyword_analysis": keyword_analysis,
                    "search_patterns": search_patterns,
                    "total_keywords_analyzed": len(keyword_analysis.get('frequent_terms', []))
                },
                "produto": context.get('produto', ''),
                "segmento": context.get('segmento', ''),
                "generated_at": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento das palavras-chave: {e}")
            return {"error": str(e), "module": "palavras_chave"}

    def _process_funil_vendas_module(self, massive_data: Dict[str, Any], context: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Processa m√≥dulo de funil de vendas usando dados massivos"""
        customer_journey = self._extract_customer_journey_insights(massive_data)
        conversion_patterns = self._extract_conversion_patterns(massive_data)
        funil_vendas_prompt = f"""
        Baseado na an√°lise massiva de dados coletados, crie um funil de vendas otimizado para "{context.get('produto', '')}" no segmento "{context.get('segmento', '')}".
        JORNADA DO CLIENTE IDENTIFICADA:
        - Pontos de contato principais: {customer_journey.get('main_touchpoints', [])}
        - Momentos de decis√£o cr√≠ticos: {customer_journey.get('decision_moments', [])}
        - Barreiras de convers√£o observadas: {customer_journey.get('conversion_barriers', [])}
        PADR√ïES DE CONVERS√ÉO:
        - Gatilhos de convers√£o eficazes: {conversion_patterns.get('conversion_triggers', [])}
        - Elementos persuasivos identificados: {conversion_patterns.get('persuasive_elements', [])}
        - Otimiza√ß√µes de convers√£o observadas: {conversion_patterns.get('conversion_optimizations', [])}
        Crie um funil que inclua:
        1. ARQUITETURA COMPLETA DO FUNIL
           - Topo (Consci√™ncia): estrat√©gias de atra√ß√£o
           - Meio (Considera√ß√£o): t√°ticas de nutri√ß√£o
           - Fundo (Decis√£o): t√©cnicas de convers√£o
           - P√≥s-venda (Reten√ß√£o): estrat√©gias de fideliza√ß√£o
        2. ESTRAT√âGIAS POR EST√ÅGIO
           - Conte√∫dos espec√≠ficos para cada etapa
           - CTAs otimizados por momento
           - Ofertas irresist√≠veis por est√°gio
           - Automatiza√ß√µes de follow-up
        3. OTIMIZA√á√ïES DE CONVERS√ÉO
           - Landing pages de alta convers√£o
           - Formul√°rios otimizados
           - Elementos de urg√™ncia e escassez
           - Provas sociais estrat√©gicas
        4. M√âTRICAS E KPIs
           - Taxa de convers√£o por est√°gio
           - Custo de aquisi√ß√£o por canal
           - Lifetime value estimado
           - ROI por investimento
        5. IMPLEMENTA√á√ÉO T√âCNICA
           - Ferramentas necess√°rias
           - Configura√ß√µes de tracking
           - Automa√ß√µes recomendadas
           - Cronograma de implementa√ß√£o
        Responda em formato JSON estruturado.
        """
        try:
            funil_vendas_result = ai_manager.generate_content(funil_vendas_prompt, max_tokens=4500)
            # Tentar parsear o resultado como JSON
            try:
                parsed_result = json.loads(funil_vendas_result)
            except json.JSONDecodeError:
                logger.error(f"‚ùå Funil vendas result is not valid JSON: {funil_vendas_result[:200]}...")
                parsed_result = {"raw_output": funil_vendas_result}
            return {
                "module": "funil_vendas",
                "funil_vendas_otimizado": parsed_result,
                "funnel_foundation": {
                    "customer_journey": customer_journey,
                    "conversion_patterns": conversion_patterns,
                    "optimization_level": "ULTRA_OTIMIZADO"
                },
                "produto": context.get('produto', ''),
                "segmento": context.get('segmento', ''),
                "generated_at": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento do funil de vendas: {e}")
            return {"error": str(e), "module": "funil_vendas"}

    def _process_insights_module(self, massive_data: Dict[str, Any], context: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Processa m√≥dulo de insights exclusivos usando dados massivos"""
        unique_insights = self._extract_unique_insights(massive_data)
        market_opportunities = self._extract_market_opportunities(massive_data)
        insights_prompt = f"""
        Baseado na an√°lise massiva de dados coletados, extraia insights exclusivos e oportunidades para "{context.get('produto', '')}" no segmento "{context.get('segmento', '')}".
        INSIGHTS √öNICOS IDENTIFICADOS:
        - Padr√µes comportamentais descobertos: {unique_insights.get('behavioral_patterns', [])}
        - Correla√ß√µes inesperadas encontradas: {unique_insights.get('unexpected_correlations', [])}
        - Tend√™ncias emergentes detectadas: {unique_insights.get('emerging_trends', [])}
        OPORTUNIDADES DE MERCADO:
        - Nichos inexplorados identificados: {market_opportunities.get('unexplored_niches', [])}
        - Necessidades n√£o atendidas: {market_opportunities.get('unmet_needs', [])}
        - Gaps competitivos descobertos: {market_opportunities.get('competitive_gaps', [])}
        Gere insights que incluam:
        1. INSIGHTS COMPORTAMENTAIS PROFUNDOS
           - 10 padr√µes comportamentais √∫nicos
           - Motiva√ß√µes ocultas descobertas
           - Gatilhos emocionais espec√≠ficos
           - Momentos de maior receptividade
        2. OPORTUNIDADES DE MERCADO EXCLUSIVAS
           - 5 nichos de alto potencial
           - Necessidades latentes identificadas
           - Gaps de produto/servi√ßo
           - Oportunidades de inova√ß√£o
        3. INSIGHTS COMPETITIVOS √öNICOS
           - Pontos cegos dos concorrentes
           - Estrat√©gias n√£o exploradas
           - Vantagens competitivas ocultas
           - Movimentos estrat√©gicos recomendados
        4. TEND√äNCIAS E PREDI√á√ïES
           - Tend√™ncias emergentes relevantes
           - Mudan√ßas comportamentais esperadas
           - Oportunidades futuras antecipadas
           - Riscos e amea√ßas identificados
        5. RECOMENDA√á√ïES ESTRAT√âGICAS
           - A√ß√µes imediatas priorit√°rias
           - Investimentos estrat√©gicos sugeridos
           - Parcerias potenciais identificadas
           - Cronograma de implementa√ß√£o
        Responda em formato JSON estruturado.
        """
        try:
            insights_result = ai_manager.generate_content(insights_prompt, max_tokens=4000)
            # Tentar parsear o resultado como JSON
            try:
                parsed_result = json.loads(insights_result)
            except json.JSONDecodeError:
                logger.error(f"‚ùå Insights result is not valid JSON: {insights_result[:200]}...")
                parsed_result = {"raw_output": insights_result}
            return {
                "module": "insights",
                "insights_exclusivos": parsed_result,
                "insights_foundation": {
                    "unique_insights": unique_insights,
                    "market_opportunities": market_opportunities,
                    "insight_quality": "PREMIUM_EXCLUSIVO"
                },
                "produto": context.get('produto', ''),
                "segmento": context.get('segmento', ''),
                "generated_at": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento dos insights: {e}")
            return {"error": str(e), "module": "insights"}

    def _process_plano_acao_module(self, massive_data: Dict[str, Any], context: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Processa m√≥dulo de plano de a√ß√£o usando dados massivos"""
        implementation_data = self._extract_implementation_insights(massive_data)
        resource_requirements = self._extract_resource_requirements(massive_data)
        plano_acao_prompt = f"""
        Baseado na an√°lise massiva de dados coletados, crie um plano de a√ß√£o detalhado para "{context.get('produto', '')}" no segmento "{context.get('segmento', '')}".
        DADOS PARA IMPLEMENTA√á√ÉO:
        - Estrat√©gias de implementa√ß√£o observadas: {implementation_data.get('implementation_strategies', [])}
        - Cronogramas de sucesso identificados: {implementation_data.get('success_timelines', [])}
        - Fatores cr√≠ticos de sucesso: {implementation_data.get('success_factors', [])}
        REQUISITOS DE RECURSOS:
        - Recursos necess√°rios t√≠picos: {resource_requirements.get('typical_resources', [])}
        - Investimentos m√©dios observados: {resource_requirements.get('investment_ranges', [])}
        - Equipes e skills necess√°rios: {resource_requirements.get('team_requirements', [])}
        Crie um plano que inclua:
        1. ROADMAP ESTRAT√âGICO (12 MESES)
           - Fase 1 (Meses 1-3): Funda√ß√£o e prepara√ß√£o
           - Fase 2 (Meses 4-6): Implementa√ß√£o e lan√ßamento
           - Fase 3 (Meses 7-9): Otimiza√ß√£o e escala
           - Fase 4 (Meses 10-12): Expans√£o e consolida√ß√£o
        2. A√á√ïES ESPEC√çFICAS POR FASE
           - Tarefas detalhadas por semana
           - Respons√°veis por cada a√ß√£o
           - Depend√™ncias entre tarefas
           - Crit√©rios de conclus√£o
        3. RECURSOS NECESS√ÅRIOS
           - Or√ßamento detalhado por categoria
           - Equipe necess√°ria e perfis
           - Ferramentas e tecnologias
           - Fornecedores e parceiros
        4. M√âTRICAS E CONTROLE
           - KPIs por fase e a√ß√£o
           - Marcos de verifica√ß√£o (milestones)
           - Indicadores de alerta precoce
           - Relat√≥rios de acompanhamento
        5. GEST√ÉO DE RISCOS
           - Principais riscos identificados
           - Planos de conting√™ncia
           - Monitoramento de riscos
           - A√ß√µes preventivas
        Responda em formato JSON estruturado.
        """
        try:
            plano_acao_result = ai_manager.generate_content(plano_acao_prompt, max_tokens=4500)
            # Tentar parsear o resultado como JSON
            try:
                parsed_result = json.loads(plano_acao_result)
            except json.JSONDecodeError:
                logger.error(f"‚ùå Plano a√ß√£o result is not valid JSON: {plano_acao_result[:200]}...")
                parsed_result = {"raw_output": plano_acao_result}
            return {
                "module": "plano_acao",
                "plano_acao_detalhado": parsed_result,
                "action_foundation": {
                    "implementation_data": implementation_data,
                    "resource_requirements": resource_requirements,
                    "planning_depth": "ULTRA_DETALHADO"
                },
                "produto": context.get('produto', ''),
                "segmento": context.get('segmento', ''),
                "generated_at": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento do plano de a√ß√£o: {e}")
            return {"error": str(e), "module": "plano_acao"}

    def _process_pre_pitch_module(self, massive_data: Dict[str, Any], context: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Processa m√≥dulo de pr√©-pitch usando dados massivos"""
        engagement_patterns = self._extract_engagement_patterns(massive_data)
        attention_triggers = self._extract_attention_triggers(massive_data)
        pre_pitch_prompt = f"""
        Baseado na an√°lise massiva de dados coletados, crie uma estrat√©gia de pr√©-pitch otimizada para "{context.get('produto', '')}" no segmento "{context.get('segmento', '')}".
        PADR√ïES DE ENGAJAMENTO IDENTIFICADOS:
        - Elementos que geram mais engajamento: {engagement_patterns.get('high_engagement_elements', [])}
        - Momentos de maior aten√ß√£o: {engagement_patterns.get('peak_attention_moments', [])}
        - Formatos mais eficazes: {engagement_patterns.get('effective_formats', [])}
        GATILHOS DE ATEN√á√ÉO:
        - Hooks mais poderosos observados: {attention_triggers.get('powerful_hooks', [])}
        - Elementos de surpresa eficazes: {attention_triggers.get('surprise_elements', [])}
        - T√©cnicas de storytelling que funcionam: {attention_triggers.get('storytelling_techniques', [])}
        Crie uma estrat√©gia que inclua:
        1. SEQU√äNCIA DE PR√â-PITCH OTIMIZADA
           - Hook de abertura irresist√≠vel
           - Pattern interrupt strategic
           - Story de identifica√ß√£o
           - Transi√ß√£o para pitch principal
        2. ELEMENTOS PSICOL√ìGICOS
           - Gatilhos de curiosidade espec√≠ficos
           - T√©cnicas de rapport instant√¢neo
           - Ancoragem emocional
           - Cria√ß√£o de urg√™ncia
        3. VARIA√á√ïES POR CANAL
           - Vers√£o para redes sociais
           - Vers√£o para email marketing
           - Vers√£o para apresenta√ß√µes
           - Vers√£o para conversas pessoais
        4. SCRIPTS DETALHADOS
           - Roteiro palavra por palavra
           - Pausas e √™nfases marcadas
           - Gestos e linguagem corporal
           - Varia√ß√µes de backup
        5. M√âTRICAS DE PERFORMANCE
           - Indicadores de engajamento
           - Taxa de convers√£o para pitch
           - Tempo de aten√ß√£o mantido
           - Feedback qualitativo
        Responda em formato JSON estruturado.
        """
        try:
            pre_pitch_result = ai_manager.generate_content(pre_pitch_prompt, max_tokens=4000)
            # Tentar parsear o resultado como JSON
            try:
                parsed_result = json.loads(pre_pitch_result)
            except json.JSONDecodeError:
                logger.error(f"‚ùå Pr√©-pitch result is not valid JSON: {pre_pitch_result[:200]}...")
                parsed_result = {"raw_output": pre_pitch_result}
            return {
                "module": "pre_pitch",
                "estrategia_pre_pitch": parsed_result,
                "pre_pitch_foundation": {
                    "engagement_patterns": engagement_patterns,
                    "attention_triggers": attention_triggers,
                    "optimization_level": "ULTRA_OTIMIZADO"
                },
                "produto": context.get('produto', ''),
                "segmento": context.get('segmento', ''),
                "generated_at": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento do pr√©-pitch: {e}")
            return {"error": str(e), "module": "pre_pitch"}

    def _save_module_json(self, module_name: str, module_data: Dict[str, Any], session_id: str):
        """Salva dados do m√≥dulo em JSON com nome do produto"""
        try:
            # Extrai nome do produto para criar nome de arquivo mais descritivo
            produto = module_data.get('produto', 'produto')
            produto_clean = produto.replace(' ', '_').replace('-', '_').lower()
            # Salva no diret√≥rio analyses_data com nome espec√≠fico
            arquivo_nome = f"{module_name}_{produto_clean}"
            salvar_etapa(arquivo_nome, module_data, categoria=module_name)
            logger.info(f"‚úÖ M√≥dulo {module_name} salvo como {arquivo_nome} em analyses_data/{module_name}/")
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar m√≥dulo {module_name}: {e}")

    def _generate_processing_summary(self, processing_results: Dict[str, Any]) -> Dict[str, Any]:
        """Gera sum√°rio do processamento"""
        modules_data = processing_results.get("modules_data", {})
        summary = {
            "total_modules_processed": len(modules_data),
            "successful_modules": len([m for m in modules_data.values() if not m.get("error")]),
            "failed_modules": len([m for m in modules_data.values() if m.get("error")]),
            "modules_list": list(modules_data.keys()),
            "processing_success_rate": 0
        }
        if summary["total_modules_processed"] > 0:
            summary["processing_success_rate"] = (summary["successful_modules"] / summary["total_modules_processed"]) * 100
        return summary

    def _extract_context_for_module(self, module_name: str, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai o contexto necess√°rio para cada m√≥dulo"""
        # Esta fun√ß√£o precisa ser implementada para extrair dados relevantes
        # do massive_data com base no module_name.
        # Por enquanto, retorna um dicion√°rio gen√©rico.
        # Em uma implementa√ß√£o real, voc√™ mapearia m√≥dulos para fontes de dados espec√≠ficas.
        logger.debug(f"Extraindo contexto para o m√≥dulo: {module_name}")
        # Tentativa de extrair informa√ß√µes de contexto mais espec√≠ficas
        context = {}
        if "product_info" in massive_data:
            context["produto"] = massive_data["product_info"].get("name", "Unknown Product")
        if "market_info" in massive_data:
            context["segmento"] = massive_data["market_info"].get("segment", "Unknown Segment")
        # Fallback para dados gen√©ricos se n√£o encontrados
        if not context.get("produto"):
            context["produto"] = "Default Product"
        if not context.get("segmento"):
            context["segmento"] = "Default Segment"
        # Adiciona um resumo dos dados massivos para ter um contexto m√≠nimo
        context["massive_data_summary"] = {
            "num_documents": len(massive_data.get("extracted_content", [])),
            "total_size": massive_data.get("statistics", {}).get("total_content_length", 0),
            "sources_count": massive_data.get("statistics", {}).get("total_sources", 0)
        }
        # Dados espec√≠ficos para palavras-chave
        if module_name == "palavras_chave":
             context.update(self._extract_keyword_analysis(massive_data))
             context.update(self._extract_search_patterns(massive_data))
        # Dados para outros m√≥dulos conforme necess√°rio
        # Ex: Se for o m√≥dulo de concorrentes:
        if module_name == "concorrencia":
            context.update(self._extract_competitive_data(massive_data))
            context.update(self._extract_market_dynamics(massive_data))
        return context

# Inst√¢ncia global
enhanced_module_processor = EnhancedModuleProcessor()
